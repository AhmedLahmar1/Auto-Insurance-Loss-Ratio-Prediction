# Auto-Insurance-Loss-Ratio-Prediction

The objective is to propose an idea to predict the loss ratio for an Auto-Insurance dataset to get a better understanding of which portfolio having different policies and many other different features can lead to a minimum/maximum loss ratio, which will in return help in the company’s profitability. What Auto-Insurance company does is, it builds a contract between the customer and the insurance company the protects the customer against financial loss in the event of an accident or theft. In exchange of the customer having to pay the premium, the insurance company will agree to pay for the losses as outlined in the policy. The auto-insurance will provide coverage for property damage, such as damage to or theft of the vehicle, liability, which is the customer’s responsibility to others for bodily injury or property damage, and lastly the medical, which is the cost of treating injuries, rehabilitation and sometimes lost wages and funeral expenses. The loss ratio is the ratio of total losses to the total premium to the amount. The loss ratio is one of the ways which helps to gauge company’s suitability for the coverage. In many cases, a high loss ratio – meaning one where the losses approach, equal, or exceeds the premium is considered bad. The aim is to optimize marketing strategies, to improve the business, to enhance the income, and to reduce costs. We propose a following modeling problem which can be carried out with the company’s data itself.
A model will be created by using the training dataset which is already available. Since the dataset is present already, supervised learning shall be carried out. The target variable has already been provided which is the natural logarithm of the loss ratio. Calculating the loss ratio is valuable because most of the policies are mispriced by more than +/- 10% up to 50%. Loss ratio can be used to increase or decrease rates. Loss reserving determines the present liability associated with the future claim payments and the estimated loss ratio can be used to determine the estimated losses. By predicting the loss ratio, it will help in assessing the profitability of the insurance company. Loss ratio reduction can be achieved three ways: increase rate, reduce losses and loss adjustment expenses, or some combination of the two. Losses in loss ratios include paid insurance claims and adjustment expenses. The loss ratio formula is insurance claims paid plus adjustment expenses divided by total earned premiums. The business collects premiums higher than the amounts paid in claims, and so if the loss ratio is high then it indicates that the business is in financial distress. Insurers set aside a portion of their premiums from underwriting new policies in order to pay for future claims. The expected loss ratio is used to determine how much they set aside. It's also important to note that the frequency and severity of the claims they expect to experience also plays a role. Maintaining a good loss ratio is important because the company gives the customers the liberty to write more policies if the ratio of payouts is low. Insurers will calculate their combined ratios, which include the loss ratio and their expense ratio, to measure total cash outflows associated with their operating activities. If loss ratios associated with the customer’s policy become excessive, an insurance provider may raise premiums or choose not to renew a policy.
In terms of data preparation, the training and testing data is readily available. This step involves iterative data exploration and transformation to make the data credible and useful. The training data consists of total 69 features. Not all the feature will contribute in predicting the target variable. Before extracting the important features, it is necessary to understand and analyze the data. Firstly, the data will be read and analyzed. Once the data has been extracted and reformatted, we must assess and correct problematic values. It’ll be followed by preprocessing which includes taking care of the null values, handling the missing or erroneous data, standardization etc. After that, to get a deeper understanding of the importance of features, we’re going to visualize the data to extract patterns and for the further data analysis. Data Transformation is also necessary for the outlier detection, missing value imputation and handling duplicate records. The correlations have also been generated to see which parameters positively or negatively impact the target variable. Then the unimportant features will be dropped. This is how we will create the feature vectors and put them into a single table.
After we’ve carried out the visualizations and the analysis, we can come to know from the patterns if it should be a linear model or whether clustering is required. For modeling, we assume it is a linear model, since all the parameters are numerical and doesn’t particularly have categories associated with it, it would be best to use regression. There are two ways we can approach the building of the model. First, by using the training data to build a model that given policy attributes predicts the loss of that individual policy. Use this model to predict the loss of all policies in the testing portfolio. Once the losses have been generated, we can compute the natural logarithm of the loss ratio on the entire test portfolio. Another approach would be, to create training portfolios and engineer a set of features that summarizes the data in that portfolio. Create a new training dataset with these features and build a model. Finally, we can predict the loss ratio from the testing dataset.
Now if we are considering the evaluation and deployment of the model, we will verify this by running the model on the database and to even verify its accuracy. We will let the stakeholders access the model for them to verify the performance, operation and effectiveness of the model. As for the evaluation and computing the model performance, we’re going to use cross-validation, which is a sophisticated holdout training and testing procedure. It estimates error over all the data by performing multiple splits and systematically swapping out samples for testing. We are going to explore trying confusion-matrix as well to assess each cost and benefits of the features impacting the target value. By this we can evaluate the generalization performance as well, just to check whether the model is evaluating correctly and to see if the overfitting doesn’t happen. And in case the overfitting does happen, then the model complexity will be decreased by extracting valuable features and choosing just that to train the model.